{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c41b0e75-4941-47d7-ba53-b7b5ba37ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from networks import SCLIPNN\n",
    "import clip\n",
    "from multilingual_clip import pt_multilingual_clip\n",
    "import transformers\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torchvision.transforms.functional as fn\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e0d803-c90f-4289-a26f-3df768bb59a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Models functions\n",
    "#######################################\n",
    "def get_sbert_and_clip_and_mclip_models():\n",
    "    sbert_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "    print(\"SBERT model loaded\")\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    print(\"CLIP model loaded\")\n",
    "    mclip_model = pt_multilingual_clip.MultilingualCLIP.from_pretrained('M-CLIP/XLM-Roberta-Large-Vit-B-32')\n",
    "    mclip_tokenizer = transformers.AutoTokenizer.from_pretrained('M-CLIP/XLM-Roberta-Large-Vit-B-32')\n",
    "    print(\"MCLIP model loaded\")\n",
    "    return sbert_model.eval(), (clip_model.eval(), preprocess), (mclip_model.eval(), mclip_tokenizer)\n",
    "\n",
    "def get_sbert_embeddings(sentences, sbert_model):\n",
    "    with torch.no_grad():  \n",
    "        sbert_embeddings = torch.from_numpy(sbert_model.encode(sentences))\n",
    "    return sbert_embeddings\n",
    "\n",
    "def get_clip_embeddings(sentences, clip_model, batch_size=10):\n",
    "    tokenized_text = clip.tokenize(sentences).to(device)\n",
    "    with torch.no_grad():\n",
    "        clip_embeddings_list = []\n",
    "        for i in range(0,tokenized_text.size()[0],batch_size):\n",
    "            tok_batch = tokenized_text[i:i+batch_size]\n",
    "            clip_embeddings_batch = clip_model.encode_text(tok_batch).to(device)\n",
    "            for unity in clip_embeddings_batch:\n",
    "                clip_embeddings_list.append(unity)\n",
    "    final_emb = torch.stack(clip_embeddings_list)\n",
    "    return final_emb\n",
    "\n",
    "def get_mclip_embeddings(sentences, mclip_model, mclip_tokenizer):\n",
    "    with torch.no_grad():\n",
    "        mclip_embeddings = mclip_model.forward(sentences, mclip_tokenizer).to(device)\n",
    "    return mclip_embeddings\n",
    "\n",
    "def load_model(path_to_model,sbert_model):\n",
    "    PATH = path_to_model\n",
    "    sbert_features = get_sbert_embeddings(['simple sentence'],sbert_model)\n",
    "    input_size = sbert_features.shape[1]\n",
    "    model = SCLIPNN(input_size,900)\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    return model\n",
    "\n",
    "def sbert_to_clip(sbert_features, name_model):\n",
    "    splitted_name = name_model.split(\"_\")\n",
    "    hidden_size = int(splitted_name[2])\n",
    "    input_size = sbert_features.shape[1]\n",
    "    PATH = os.path.join(\"models\",name_model)\n",
    "    model = SCLIPNN(input_size,hidden_size)\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.eval()\n",
    "    output = model(sbert_features)\n",
    "    return output\n",
    "\n",
    "def get_logits(image_features, text_features, encoding='sbert'):\n",
    "    # normalized features\n",
    "    if text_features.dtype == torch.int64:\n",
    "        text_features = text_features.type(torch.FloatTensor)\n",
    "    if text_features.dtype == torch.float32:\n",
    "        text_features = text_features.to(torch.float16)\n",
    "    if text_features.dtype == torch.float16:\n",
    "        text_features = text_features.to(torch.float32)\n",
    "        \n",
    "    image_features = (image_features / image_features.norm(dim=-1, keepdim=True)).to(device)\n",
    "    text_features = (text_features / text_features.norm(dim=-1, keepdim=True)).to(device)\n",
    "\n",
    "    # cosine similarity as logits\n",
    "    logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "    logit_scale = logit_scale.exp().to(device)\n",
    "    logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "    logits_per_text = logits_per_image.t()\n",
    "\n",
    "    # shape = [global_batch_size, global_batch_size]\n",
    "    return logits_per_image, logits_per_text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4136551c-1a27-4031-a674-86c95caa9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Images functions\n",
    "#################################\n",
    "def get_image(directory, image_id):\n",
    "    image = Image.open(os.path.join(directory, image_id))\n",
    "    return image\n",
    "\n",
    "def reshape(im):\n",
    "    print(\"This is size of original image:\",im.size, \"\\n\")\n",
    "    width, height = im.size\n",
    "    # print(\"W: {} and H: {}\".format(width, height))\n",
    "    if width > 1000 or height > 1000:\n",
    "        scale = 3\n",
    "    elif width > 500 or height > 500:\n",
    "        scale = 2\n",
    "    else:\n",
    "        scale = 1    \n",
    "    new_width = int(width / scale)\n",
    "    new_height = int(height / scale)\n",
    "    #image = preprocess(im)\n",
    "    image = fn.resize(im, size=[new_width])\n",
    "    print(\"This is size of resized image:\",image.size, \"\\n\")\n",
    "    return image\n",
    "\n",
    "def get_image_features(images, image_directory, clip_model, preprocess):\n",
    "    N = len(images)\n",
    "    count = 0\n",
    "    image_features = torch.empty(size=(N, 512))\n",
    "    for i,image_id in enumerate(images):\n",
    "        count += 1\n",
    "        im = get_image(image_directory, image_id)\n",
    "        image = preprocess(im).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            clip_image = clip_model.encode_image(image)\n",
    "            image_features[i] = clip_image\n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4938910-b9a3-48c1-bccf-eb640313c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_and_captions(languages):\n",
    "    images_of_language = {}\n",
    "    captions_of_language = {}\n",
    "    for lang, code in languages.items():        \n",
    "        f_json = open(os.path.join(directory, \"{}_pairs.json\".format(code)), mode='r', encoding='utf-8')\n",
    "        pairs_data = json.load(f_json)\n",
    "        images = []\n",
    "        captions = []\n",
    "        for pair in pairs_data:\n",
    "            images.append(pair[\"image_id\"])\n",
    "            captions.append(pair[\"caption\"])\n",
    "        images_of_language[lang] = images\n",
    "        captions_of_language[lang] = captions\n",
    "    return images_of_language, captions_of_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71fd2981-c14f-4353-bf9b-3becbeaa133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_and_captions_clip_features(languages, image_directory,clip_model, preprocess):\n",
    "    images, captions = get_images_and_captions(languages)\n",
    "    images_features = {}\n",
    "    clip_features = {}\n",
    "    for lang in languages.keys():\n",
    "        images_features[lang] = get_image_features(images[lang],image_directory,clip_model, preprocess)\n",
    "        clip_features[lang] = get_clip_features(captions[lang],clip_model).to(device)\n",
    "    return images_features, clip_features, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53429d7-2d6a-4bec-bd7d-af8648e324a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Additional Functions\n",
    "#######################################\n",
    "def display_results(sbert_lang_performance, clip_lang_performance, sbert_lang_errors, clip_lang_errors, sbert_lang_mrr, clip_lang_mrr):\n",
    "    results = pd.DataFrame({\"SBERT\":sbert_lang_performance, \"CLIP\": clip_lang_performance,\n",
    "                        \"error SBERT\":sbert_lang_errors, \"error CLIP\":clip_lang_errors,\n",
    "                       \"MRR sbert\":sbert_lang_mrr, \"MRR clip\": clip_lang_mrr}, \n",
    "                       index=languages)\n",
    "    print(results)\n",
    "\n",
    "def show_plot(languages, sbert_lang_mrr, clip_lang_mrr, mclip_lang_mrr):\n",
    "    X_axis = np.arange(len(languages.keys()))\n",
    "    figure_name = plt.figure(figsize=(20, 8))\n",
    "    plt.bar(X_axis-0.2, sbert_lang_mrr, 0.4, color = 'blue', edgecolor = 'black', capsize=7, label='SBERT MRR')\n",
    "    plt.bar(X_axis+0.2, clip_lang_mrr, 0.4, color = 'red', edgecolor = 'black', capsize=7, label='CLIP MRR')\n",
    "    plt.bar(X_axis+0.2, mclip_lang_mrr, 0.4, color = 'yellow', edgecolor = 'black', capsize=7, label='CLIP MRR')\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.xticks(X_axis, languages.keys())\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def reciprocal_rank(probs, value):\n",
    "    N = len(probs)\n",
    "    copy_probs = list(probs.copy())\n",
    "    for i in range(N):\n",
    "        max_value = max(copy_probs)\n",
    "        if max_value == value:\n",
    "            return 1/(i + 1)\n",
    "        else:\n",
    "            copy_probs.remove(max_value)\n",
    "    return 1/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54af2b55-336c-48c0-b1da-5d6f8c807b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MRR(languages, name_of_model, sbert_model, clip_model, preprocess, mclip_model, mclip_tokenizer, captions, images_features):\n",
    "    sbert_lang_performance = []\n",
    "    clip_lang_performance = []\n",
    "    mclip_lang_performance = []\n",
    "    sbert_lang_errors = []\n",
    "    clip_lang_errors = []\n",
    "    mclip_lang_errors = []\n",
    "    sbert_lang_mrr = []\n",
    "    clip_lang_mrr = []\n",
    "    mclip_lang_mrr = []\n",
    "    vetoed = []\n",
    "    for lang, code in languages.items():\n",
    "        print(\"Processing captions in \"+ lang +\"...\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                torch_features = get_sbert_embeddings(captions[lang],sbert_model) \n",
    "                sbert_features = sbert_to_clip(torch_features,name_of_model).type(torch.float16)\n",
    "                print(\"SBERT features ready. Timestamp: {}\".format(datetime.now()))\n",
    "                clip_features = get_clip_embeddings(captions[lang],clip_model).to(device)\n",
    "                print(\"CLIP features ready. Timestamp: {}\".format(datetime.now()))\n",
    "                mclip_features = get_mclip_embeddings(captions[lang],mclip_model,mclip_tokenizer)\n",
    "                print(\"MCLIP features ready. Timestamp: {}\".format(datetime.now()))\n",
    "            except:\n",
    "                print(\"Not able to tokenize in {}. Skipping language {}\".format(lang, code))\n",
    "                vetoed.append(lang)\n",
    "                continue\n",
    "            \n",
    "        print(\"Encodings complete\")\n",
    "\n",
    "        sbert_performance = []\n",
    "        clip_performance = []\n",
    "        mclip_performance = []\n",
    "        sbert_errors = 0\n",
    "        clip_errors = 0\n",
    "        mclip_errors = 0\n",
    "        sbert_rr = 0\n",
    "        clip_rr = 0\n",
    "        mclip_rr = 0\n",
    "        counter = 0\n",
    "        \n",
    "        for image_features in images_features:\n",
    "            # Get the probabilities for SBERT and CLIP\n",
    "            logits_image_sbert, logits_text_sbert = get_logits(image_features, sbert_features,'sbert')\n",
    "            logits_image_mclip, logits_text_mclip = get_logits(image_features, mclip_features,'mclip')\n",
    "            logits_image_clip, logits_text_clip = get_logits(image_features, clip_features,'clip')\n",
    "            probs_sbert = logits_image_sbert.softmax(dim=-1).cpu().detach().numpy()\n",
    "            probs_mclip = logits_image_mclip.softmax(dim=-1).cpu().detach().numpy()\n",
    "            probs_clip = logits_image_clip.softmax(dim=-1).cpu().detach().numpy()\n",
    "            \n",
    "            # Append the probs to array            \n",
    "            ps = probs_sbert[counter]\n",
    "            sbert_rr += reciprocal_rank(probs_sbert,ps)\n",
    "            sbert_performance.append(ps)\n",
    "            if ps < max(probs_sbert):\n",
    "                sbert_errors += 1\n",
    "            pc = probs_clip[counter]\n",
    "            clip_rr += reciprocal_rank(probs_clip,pc)\n",
    "            clip_performance.append(pc)\n",
    "            if pc < max(probs_clip):\n",
    "                clip_errors += 1\n",
    "            pm = probs_mclip[counter]\n",
    "            mclip_rr += reciprocal_rank(probs_mclip,pm)\n",
    "            mclip_performance.append(pm)\n",
    "            if pm < max(probs_mclip):\n",
    "                mclip_errors += 1\n",
    "            counter += 1\n",
    "\n",
    "            if counter % 100 == 0:\n",
    "                print(\"{} images already processed for {}\".format(counter,lang))\n",
    "\n",
    "        print('-'*70)\n",
    "        # print(\"Images processed: {}\".format(counter))\n",
    "        # print(\"Classifications errors: SBERT --> {} ; CLIP --> {}\".format(sbert_errors,clip_errors))\n",
    "        sbert_lang_performance.append(round(sum(sbert_performance)/counter,6))\n",
    "        clip_lang_performance.append(round(sum(clip_performance)/counter,6))\n",
    "        mclip_lang_performance.append(round(sum(mclip_performance)/counter,6))\n",
    "        sbert_lang_mrr.append(round(sbert_rr/counter,4))\n",
    "        clip_lang_mrr.append(round(clip_rr/counter,4))\n",
    "        mclip_lang_mrr.append(round(mclip_rr/counter,4))\n",
    "        sbert_lang_errors.append(sbert_errors)\n",
    "        clip_lang_errors.append(clip_errors)\n",
    "        mclip_lang_errors.append(mclip_errors)\n",
    "    print(\"Done\")\n",
    "    print(\"Forbidden Languages: {}\".format(vetoed))\n",
    "    \n",
    "    return sbert_lang_performance, clip_lang_performance, mclip_lang_performance, sbert_lang_mrr, clip_lang_mrr, mclip_lang_mrr, sbert_lang_errors, clip_lang_errors, mclip_lang_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "953f5d46-2521-4b5a-93bb-e56ade41246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image_directory: /home/users/acordova/project/SCLIP/coco/images/val2017\n",
      "{'english': 'en', 'spanish': 'es', 'italian': 'it', 'german': 'de', 'french': 'fr'}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    with open(os.path.join(\"preprocessing\", \"config.yml\"), \"r\") as ymlfile:\n",
    "        cfg = yaml.safe_load(ymlfile)\n",
    "    directory = cfg[\"coco\"][\"out_dir\"]\n",
    "    image_directory = cfg[\"coco\"][\"image_dir\"]\n",
    "    print(\"Image_directory: {}\".format(image_directory))\n",
    "    languages = cfg[\"languages\"]\n",
    "    model_dir = cfg[\"models\"][\"model_dir\"]\n",
    "    name_of_model = 'coco_NN_900_e300_s400000.pt'\n",
    "    trained_model = os.path.join(model_dir,name_of_model)\n",
    "    print(languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "414c4a9c-5b36-49b9-8c8c-d2278077f529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT model loaded\n",
      "CLIP model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCLIP model loaded\n"
     ]
    }
   ],
   "source": [
    "sbert_model, (clip_model, preprocess), (mclip_model, mclip_tokenizer) = get_sbert_and_clip_and_mclip_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d75d88f-08f3-43e1-9179-8222377c5281",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, captions = get_images_and_captions(languages)\n",
    "images_features = get_image_features(images[\"english\"], image_directory, clip_model, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c695a11-011a-4ebc-b36e-75178fc4ef33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing captions in english...\n",
      "SBERT features ready. Timestamp: 2022-12-09 10:42:23.936047\n",
      "CLIP features ready. Timestamp: 2022-12-09 10:42:25.543090\n",
      "MCLIP features ready. Timestamp: 2022-12-09 10:46:04.616374\n",
      "Encodings complete\n",
      "100 images already processed for english\n",
      "200 images already processed for english\n",
      "300 images already processed for english\n",
      "400 images already processed for english\n",
      "----------------------------------------------------------------------\n",
      "Processing captions in spanish...\n",
      "SBERT features ready. Timestamp: 2022-12-09 10:46:07.393244\n",
      "CLIP features ready. Timestamp: 2022-12-09 10:46:09.004198\n",
      "MCLIP features ready. Timestamp: 2022-12-09 10:49:41.820665\n",
      "Encodings complete\n",
      "100 images already processed for spanish\n",
      "200 images already processed for spanish\n",
      "300 images already processed for spanish\n",
      "400 images already processed for spanish\n",
      "----------------------------------------------------------------------\n",
      "Processing captions in italian...\n",
      "SBERT features ready. Timestamp: 2022-12-09 10:49:44.871484\n",
      "CLIP features ready. Timestamp: 2022-12-09 10:49:46.495045\n",
      "MCLIP features ready. Timestamp: 2022-12-09 10:53:19.100895\n",
      "Encodings complete\n",
      "100 images already processed for italian\n",
      "200 images already processed for italian\n",
      "300 images already processed for italian\n",
      "400 images already processed for italian\n",
      "----------------------------------------------------------------------\n",
      "Processing captions in german...\n",
      "SBERT features ready. Timestamp: 2022-12-09 10:53:22.237969\n",
      "CLIP features ready. Timestamp: 2022-12-09 10:53:23.881875\n",
      "MCLIP features ready. Timestamp: 2022-12-09 10:57:24.469215\n",
      "Encodings complete\n",
      "100 images already processed for german\n",
      "200 images already processed for german\n",
      "300 images already processed for german\n",
      "400 images already processed for german\n",
      "----------------------------------------------------------------------\n",
      "Processing captions in french...\n",
      "SBERT features ready. Timestamp: 2022-12-09 10:57:27.603376\n",
      "CLIP features ready. Timestamp: 2022-12-09 10:57:29.251006\n",
      "MCLIP features ready. Timestamp: 2022-12-09 11:01:26.047204\n",
      "Encodings complete\n",
      "100 images already processed for french\n",
      "200 images already processed for french\n",
      "300 images already processed for french\n",
      "400 images already processed for french\n",
      "----------------------------------------------------------------------\n",
      "Done\n",
      "Forbidden Languages: []\n"
     ]
    }
   ],
   "source": [
    "sbert_per, clip_per, mclip_per, sbert_MRR, clip_MRR, mclip_MRR, sbert_errors, clip_errors, mclip_errors = get_MRR(languages, trained_model, sbert_model, clip_model, preprocess, mclip_model, mclip_tokenizer, captions, images_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3ec37ab-ff99-43f1-b2c5-2322fa5ac979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            SBERT      CLIP  error SBERT  error CLIP  MRR sbert  MRR clip\n",
      "english  0.010901  0.015532          271         220     0.5242    0.6257\n",
      "spanish  0.009957  0.006618          295         327     0.4793    0.3837\n",
      "italian  0.009801  0.005680          303         363     0.4622    0.3070\n",
      "german   0.010233  0.005519          293         352     0.4822    0.3197\n",
      "french   0.010251  0.007009          297         326     0.4791    0.3894\n"
     ]
    }
   ],
   "source": [
    "display_results(sbert_per,clip_per,sbert_errors, clip_errors,sbert_MRR,clip_MRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fe3fa9f-a997-4434-8931-a904d424595c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_axis = np.arange(len(languages.keys()))\n",
    "figure_name = plt.figure(figsize=(20, 8))\n",
    "plt.bar(X_axis-0.0, clip_MRR, 0.2, color = 'red', edgecolor = 'black', capsize=7, label='CLIP MRR')\n",
    "plt.bar(X_axis-0.2, sbert_MRR, 0.2, color = 'blue', edgecolor = 'black', capsize=7, label='SBERT MRR')\n",
    "plt.bar(X_axis+0.2, mclip_MRR, 0.2, color = 'yellow', edgecolor = 'black', capsize=7, label='MCLIP MRR')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.xticks(X_axis, languages.keys())\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e7eb6e-cbe0-41df-9784-06bcde7eee4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
