{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c41b0e75-4941-47d7-ba53-b7b5ba37ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from networks import SCLIPNN\n",
    "import clip\n",
    "from multilingual_clip import pt_multilingual_clip\n",
    "import transformers\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torchvision.transforms.functional as fn\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ff47409-013f-41ab-ae9e-ba852d78220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "with open(os.path.join(\"preprocessing\", \"config.yml\"), \"r\") as ymlfile:\n",
    "    cfg = yaml.safe_load(ymlfile)\n",
    "directory = cfg[\"coco\"][\"out_dir\"]\n",
    "image_directory = cfg[\"coco\"][\"image_dir\"]\n",
    "languages = cfg[\"languages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e54e1db2-82e3-457c-96fb-3ab2119cd11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(directory, image_id):\n",
    "    image = Image.open(os.path.join(directory, image_id))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71fd2981-c14f-4353-bf9b-3becbeaa133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(images, image_directory, clip_model, preprocess):\n",
    "    image_features = []\n",
    "    count = 0\n",
    "    for image_id in images:\n",
    "        count += 1\n",
    "        im = get_image(image_directory, image_id)\n",
    "        image = preprocess(im).unsqueeze(0).to(device)\n",
    "        clip_image = clip_model.encode_image(image).to(\"cpu\")\n",
    "        image_features.append(clip_image)\n",
    "        if count % 125 == 0:\n",
    "            print(count)\n",
    "    #    if count % 125 == 0:\n",
    "    #        print(f'DEBUG EXPERIMENT. Count: {count}, image_id: {image_id}')\n",
    "    #        print(f'Types: image_id {type(image_id)}, im: {type(im)}, image: {type(image)}')\n",
    "    #        print(f'IM: {im.size}.')\n",
    "    #        print(f'Image: {image.size()}')\n",
    "    final_emb = torch.stack(image_features)\n",
    "    return image_features\n",
    "\n",
    "def get_clip_features(sentences, clip_model, batch_size=10):\n",
    "    tokenized_text = clip.tokenize(sentences).to(device)\n",
    "    with torch.no_grad():\n",
    "        clip_embeddings_list = []\n",
    "        for i in range(0,tokenized_text.size()[0],batch_size):\n",
    "            tok_batch = tokenized_text[i:i+batch_size]\n",
    "            clip_embeddings_batch = clip_model.encode_text(tok_batch).to(device)\n",
    "            for unity in clip_embeddings_batch:\n",
    "                clip_embeddings_list.append(unity)\n",
    "    final_emb = torch.stack(clip_embeddings_list)\n",
    "    return final_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e0d803-c90f-4289-a26f-3df768bb59a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sbert_and_clip_and_mclip_models():\n",
    "    sbert_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    mclip_model = pt_multilingual_clip.MultilingualCLIP.from_pretrained('M-CLIP/XLM-Roberta-Large-Vit-B-32')\n",
    "    mclip_tokenizer = transformers.AutoTokenizer.from_pretrained('M-CLIP/XLM-Roberta-Large-Vit-B-32')\n",
    "    return sbert_model.eval(), (clip_model.eval(), preprocess), (mclip_model, mclip_tokenizer)\n",
    "\n",
    "def get_sbert_embeddings(sentences, sbert_model):\n",
    "    with torch.no_grad():  \n",
    "        sbert_embeddings = torch.from_numpy(sbert_model.encode(sentences))\n",
    "    return sbert_embeddings\n",
    "\n",
    "def load_model(path_to_model,sbert_model):\n",
    "    PATH = path_to_mode\n",
    "    sbert_features = get_sbert_embeddings(['simple sentence'],sbert_model)\n",
    "    input_size = sbert_features.shape[1]\n",
    "    model = SCLIPNN(input_size,900)\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4938910-b9a3-48c1-bccf-eb640313c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_and_captions(languages):\n",
    "    images_of_language = {}\n",
    "    captions_of_language = {}\n",
    "    for lang, code in languages.items():        \n",
    "        f_json = open(os.path.join(directory, \"{}_pairs.json\".format(code)), mode='r', encoding='utf-8')\n",
    "        pairs_data = json.load(f_json)\n",
    "        images = []\n",
    "        captions = []\n",
    "        for pair in pairs_data:\n",
    "            images.append(pair[\"image_id\"])\n",
    "            captions.append(pair[\"caption\"])\n",
    "        images_of_language[lang] = images\n",
    "        captions_of_language[lang] = captions\n",
    "    return images_of_language, captions_of_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4136551c-1a27-4031-a674-86c95caa9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(im):\n",
    "    print(\"This is size of original image:\",im.size, \"\\n\")\n",
    "    width, height = im.size\n",
    "    # print(\"W: {} and H: {}\".format(width, height))\n",
    "    if width > 1000 or height > 1000:\n",
    "        scale = 3\n",
    "    elif width > 500 or height > 500:\n",
    "        scale = 2\n",
    "    else:\n",
    "        scale = 1    \n",
    "    new_width = int(width / scale)\n",
    "    new_height = int(height / scale)\n",
    "    #image = preprocess(im)\n",
    "    image = fn.resize(im, size=[new_width])\n",
    "    print(\"This is size of resized image:\",image.size, \"\\n\")\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79772c29-dc8c-4eef-aa32-9424ebdc4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(image_features, text_features, encoding='sbert'):\n",
    "    # normalized features\n",
    "    if text_features.dtype == torch.int64:\n",
    "        text_features = text_features.type(torch.FloatTensor)\n",
    "    if text_features.dtype == torch.float32:\n",
    "        text_features = text_features.to(torch.float16)\n",
    "        \n",
    "    image_features = (image_features / image_features.norm(dim=-1, keepdim=True)).to(device)\n",
    "    text_features = (text_features / text_features.norm(dim=-1, keepdim=True)).to(device)\n",
    "\n",
    "    # cosine similarity as logits\n",
    "    logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "    logit_scale = logit_scale.exp().to(device)\n",
    "    #print(\"Type for Encoding {}: Image {} and Text {}\".format(encoding,image_features.dtype,text_features.dtype))\n",
    "    logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "    logits_per_text = logits_per_image.t()\n",
    "\n",
    "    # shape = [global_batch_size, global_batch_size]\n",
    "    return logits_per_image, logits_per_text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13ff8796-2621-4b54-ae46-c2501ca069f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbert_to_clip(sbert_features, name_model):\n",
    "    splitted_name = name_model.split(\"_\")\n",
    "    hidden_size = int(splitted_name[2])\n",
    "    input_size = sbert_features.shape[1]\n",
    "    PATH = os.path.join(\"models\",name_model)\n",
    "    model = SCLIPNN(input_size,hidden_size)\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.eval()\n",
    "    output = model(sbert_features)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a53429d7-2d6a-4bec-bd7d-af8648e324a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank(probs, value):\n",
    "    N = len(probs)\n",
    "    copy_probs = list(probs.copy())\n",
    "    for i in range(N):\n",
    "        max_value = max(copy_probs)\n",
    "        if max_value == value:\n",
    "            return 1/(i + 1)\n",
    "        else:\n",
    "            copy_probs.remove(max_value)\n",
    "    return 1/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54af2b55-336c-48c0-b1da-5d6f8c807b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MRR():\n",
    "    sbert_lang_performance = []\n",
    "    clip_lang_performance = []\n",
    "    mclip_lang_performance = []\n",
    "    sbert_lang_errors = []\n",
    "    clip_lang_errors = []\n",
    "    mclip_lang_errors = []\n",
    "    sbert_lang_mrr = []\n",
    "    clip_lang_mrr = []\n",
    "    mclip_lang_mrr = []\n",
    "    vetoed = []\n",
    "    for lang, code in languages.items():\n",
    "        print(\"Processing captions in \"+ lang +\"...\")\n",
    "        f_json =  open(directory + code + \"_pairs.json\",mode='r',encoding='utf-8')\n",
    "        pairs_data = json.load(f_json)\n",
    "        images = []\n",
    "        captions = []\n",
    "        for pair in pairs_data:\n",
    "            images.append(pair[\"image_id\"])\n",
    "            captions.append(pair[\"caption\"])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                torch_features = torch.from_numpy(sbert_model.encode(captions)) \n",
    "                sbert_features = sbert_to_clip(torch_features,name_of_model).type(torch.float16)\n",
    "                tokenized_features = clip.tokenize(captions).to(device)\n",
    "                clip_features = clip_model.encode_text(tokenized_features)\n",
    "            except:\n",
    "                print(\"SBERT1: Not able to tokenize in {}. Skipping language {}\".format(lang, code))\n",
    "                vetoed.append(lang)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                mclip_features = mclip_model.forward(captions, mclip_tokenizer)\n",
    "            except:\n",
    "                print(\"MCLIP: Not able to tokenize in {}. Skipping language {}\".format(lang, code))\n",
    "                vetoed.append(lang)\n",
    "                continue\n",
    "\n",
    "            print(\"Encodings complete\")\n",
    "\n",
    "            sbert_performance = []\n",
    "            clip_performance = []\n",
    "            mclip_performance = []\n",
    "            sbert_errors = 0\n",
    "            clip_errors = 0\n",
    "            mclip_errors = 0\n",
    "            sbert_rr = 0\n",
    "            clip_rr = 0\n",
    "            mclip_rr = 0\n",
    "            counter = 0\n",
    "\n",
    "            for image_id in images:\n",
    "                # Get the encoding of the image\n",
    "                im = get_image(image_directory, image_id)\n",
    "                image = preprocess(im).unsqueeze(0).to(device)\n",
    "                image_features = clip_model.encode_image(image)\n",
    "\n",
    "                print(\"Images encoded\")\n",
    "\n",
    "                # Get the probabilities for SBERT and CLIP\n",
    "                logits_image_sbert, logits_text_sbert = get_logits(image_features, sbert_features,'sbert')\n",
    "                logits_image_mclip, logits_text_mclip = get_logits(image_features, mclip_features,'mclip')\n",
    "                logits_image_clip, logits_text_clip = get_logits(image_features, clip_features,'clip')\n",
    "                probs_clip = logits_image_clip.softmax(dim=-1).cpu().numpy()\n",
    "                probs_sbert = logits_image_sbert.softmax(dim=-1).cpu().numpy()\n",
    "                probs_mclip = logits_image_mclip.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "                # Append the probs to array            \n",
    "                ps = probs_sbert[0][counter]\n",
    "                sbert_rr += reciprocal_rank(probs_sbert[0],ps)\n",
    "                sbert_performance.append(ps)\n",
    "                if ps < max(probs_sbert[0]):\n",
    "                    sbert_errors += 1\n",
    "                pc = probs_clip[0][counter]\n",
    "                clip_rr += reciprocal_rank(probs_clip[0],pc)\n",
    "                clip_performance.append(pc)\n",
    "                if pc < max(probs_clip[0]):\n",
    "                    clip_errors += 1\n",
    "                pm = probs_mclip[0][counter]\n",
    "                mclip_rr += reciprocal_rank(probs_mclip[0],pm)\n",
    "                mclip_performance.append(pm)\n",
    "                if pm < max(probs_mclip[0]):\n",
    "                    mclip_errors += 1\n",
    "                counter += 1\n",
    "\n",
    "                if counter % 10 == 0:\n",
    "                    print(\"{} images already processed for {}\".format(counter,lang))\n",
    "\n",
    "        # print(\"Images processed: {}\".format(counter))\n",
    "        # print(\"Classifications errors: SBERT --> {} ; CLIP --> {}\".format(sbert_errors,clip_errors))\n",
    "        sbert_lang_performance.append(round(sum(sbert_performance)/counter,6))\n",
    "        clip_lang_performance.append(round(sum(clip_performance)/counter,6))\n",
    "        mclip_lang_performance.append(round(sum(mclip_performance)/counter,6))\n",
    "        sbert_lang_mrr.append(round(sbert_rr/counter,4))\n",
    "        clip_lang_mrr.append(round(clip_rr/counter,4))\n",
    "        mclip_lang_mrr.append(round(mclip_rr/counter,4))\n",
    "        sbert_lang_errors.append(sbert_errors)\n",
    "        clip_lang_errors.append(clip_errors)\n",
    "        mclip_lang_errors.append(mclip_errors)\n",
    "    print(\"Done\")\n",
    "    print(\"Forbidden Languages: {}\".format(vetoed))\n",
    "    \n",
    "    return sbert_lang_performance, clip_lang_performance, mclip_lang_performance, sbert_lang_mrr, clip_lang_mrr, mclip_lang_mrr, sbert_lang_errors, clip_lang_errors, mclip_lang_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10dbca7b-b7d7-48a2-872e-7e30ca9d399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_and_captions_clip_features(languages, image_directory,clip_model, preprocess):\n",
    "    images, captions = get_images_and_captions(languages)\n",
    "    images_features = {}\n",
    "    clip_features = {}\n",
    "    for lang in languages.keys():\n",
    "        images_features[lang] = get_image_features(images[lang],image_directory,clip_model, preprocess)\n",
    "        clip_features[lang] = get_clip_features(captions[lang],clip_model).to(device)\n",
    "    return images_features, clip_features, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "953f5d46-2521-4b5a-93bb-e56ade41246d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-dac67e90a89c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sbert_and_clip_and_mclip_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimages_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_and_captions_clip_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclip_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/coco_NN_900_e300_s400000.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msbert_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msbert_MRR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_MRR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msbert_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_MRR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msbert_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclip_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ef2bd989cca5>\u001b[0m in \u001b[0;36mget_image_and_captions_clip_features\u001b[0;34m(languages, image_directory, clip_model, preprocess)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclip_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mimages_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_directory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclip_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mclip_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_clip_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclip_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimages_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3bf1c04cbb67>\u001b[0m in \u001b[0;36mget_image_features\u001b[0;34m(images, image_directory, clip_model, preprocess)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mclip_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mimage_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    sbert_model, clip_model, preprocess = get_sbert_and_clip_and_mclip_models()\n",
    "    images_features, clip_features, captions = get_image_and_captions_clip_features(languages, image_directory,clip_model, preprocess)\n",
    "    model = load_model('models/coco_NN_900_e300_s400000.pt'),\n",
    "    sbert_per, clip_per, sbert_MRR, clip_MRR, sbert_errors, clip_errors = get_MRR(model,directory, languages,sbert_model,captions, images_features,clip_features)\n",
    "    display_results(sbert_per,clip_per,sbert_errors, clip_errors,sbert_MRR,clip_MRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03f78e0-4bc5-4a09-b443-9de09aac0aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = dict(languages)\n",
    "for k in vetoed:\n",
    "    if k in r:\n",
    "        del r[k]\n",
    "languages = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe3fa9f-a997-4434-8931-a904d424595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Languages: {}\".format(languages))\n",
    "print(\"SBERT performance: {}\".format(sbert_lang_performance))\n",
    "print(\"CLIP performance: {}\".format(clip_lang_performance))\n",
    "print(\"MCLIP performance: {}\".format(mclip_lang_performance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1932f62-4f8b-409d-9c5b-fc865d9ef496",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"SBERT\":sbert_lang_performance, \"CLIP\": clip_lang_performance, \"MCLIP\": mclip_lang_performance,\n",
    "                        \"error SBERT\":sbert_lang_errors, \"error CLIP\":clip_lang_errors, \"error MCLIP\":mclip_lang_errors,\n",
    "                       \"MRR sbert\":sbert_lang_mrr, \"MRR clip\": clip_lang_mrr, \"MRR mclip\": mclip_lang_mrr}, \n",
    "                       index=languages)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100d7270-4bf3-4c8a-a52e-f0dca470b148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "X_axis = np.arange(len(languages.keys()))\n",
    "figure_name = plt.figure(figsize=(20, 8))\n",
    "plt.bar(X_axis-0.2, sbert_lang_mrr, 0.4, color = 'blue', edgecolor = 'black', capsize=7, label='SBERT MRR')\n",
    "plt.bar(X_axis+0.2, clip_lang_mrr, 0.4, color = 'red', edgecolor = 'black', capsize=7, label='CLIP MRR')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.xticks(X_axis, languages.keys())\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c616011-266d-4841-90c4-e338c01dae82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b6712-d7da-4eee-8078-c42b98259387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
