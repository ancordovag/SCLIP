{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fa584e1-a0a3-4e17-82ad-d9deeed0f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from networks import SCLIPNN\n",
    "import clip\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c9fb9-988f-494a-adca-4f6cf1b3c7d0",
   "metadata": {},
   "source": [
    "We can now test what happens if you use a multilingual SBERT model and put in a Spanish sentence and map it to CLIP and then compare it against a image put into CLIP. Maybe you can first just test some examples using arbitrary example images. I.e. try a Spanish sentence with different images (some that match the sentence well, some that match it less well, some that don't match it at all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7707f43d-fdbd-4574-bc69-5c12d2be5b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "spanish_sentences = [\"un perro blanco\", \"un animal grande\", \"la muchacha de pelo azul\"]\n",
    "sbert_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d81bbea2-3cf1-4100-82dd-159f30de11e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "image = preprocess(Image.open(\"imgs/poodle.jpg\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize(spanish_sentences).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebd973b3-5ef0-472c-a251-363c3dda455f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Before: Type: <class 'torch.Tensor'>, Shape: torch.Size([3, 77]), Text: tensor([49406,  2271,   703,  2795, 26801], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Text Before: Type: {}, Shape: {}, Text: {}\".format(type(text), text.shape, text[0][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fd450e1-9fc2-4b27-9586-ead334abb0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text After: Type: <class 'torch.Tensor'>, Shape: torch.Size([3, 77]), Text: tensor([49406,  2271,   703,  2795, 26801], device='cuda:0')\n",
      "512\n",
      "Label probs: [[0.967   0.02347 0.00963]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    image_features = clip_model.encode_image(image)\n",
    "    sbert_features = torch.from_numpy(sbert_model.encode(spanish_sentences))\n",
    "    clip_features = clip_model.encode_text(text)\n",
    "    print(\"Text After: Type: {}, Shape: {}, Text: {}\".format(type(text), text.shape, text[0][:5]))\n",
    "    logits_per_image, logits_per_text = clip_model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "input_size = sbert_features.shape[1]\n",
    "print(input_size)\n",
    "PATH = \"models/best_model.pt\"\n",
    "model = SCLIPNN(input_size,850)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3669d505-bc3a-403d-8043-168f775b953a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
