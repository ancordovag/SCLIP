{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from networks import SCLIPNN\n",
    "import clip\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torchvision.transforms.functional as fn\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Models functions\n",
    "#######################################\n",
    "def get_sbert_and_clip_models():\n",
    "    sbert_model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "    print(\"SBERT model loaded\")\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    print(\"CLIP model loaded\")\n",
    "    return sbert_model.eval(), clip_model.eval(), preprocess\n",
    "\n",
    "def get_sbert_embeddings(sentences, sbert_model):\n",
    "    with torch.no_grad():  \n",
    "        sbert_embeddings = torch.from_numpy(sbert_model.encode(sentences))\n",
    "    return sbert_embeddings\n",
    "\n",
    "def get_clip_embeddings(sentences, clip_model, batch_size=10):\n",
    "    tokenized_text = clip.tokenize(sentences).to(device)\n",
    "    with torch.no_grad():\n",
    "        clip_embeddings_list = []\n",
    "        for i in range(0,tokenized_text.size()[0],batch_size):\n",
    "            tok_batch = tokenized_text[i:i+batch_size]\n",
    "            clip_embeddings_batch = clip_model.encode_text(tok_batch).to(device)\n",
    "            for unity in clip_embeddings_batch:\n",
    "                clip_embeddings_list.append(unity)\n",
    "    final_emb = torch.stack(clip_embeddings_list)\n",
    "    return final_emb\n",
    "\n",
    "def load_model(path_to_model,sbert_model):\n",
    "    PATH = path_to_model\n",
    "    sbert_features = get_sbert_embeddings(['simple sentence'],sbert_model)\n",
    "    input_size = sbert_features.shape[1]\n",
    "    model = SCLIPNN(input_size,900)\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    return model\n",
    "\n",
    "def sbert_to_clip(sbert_features, name_model):\n",
    "    splitted_name = name_model.split(\"_\")\n",
    "    hidden_size = int(splitted_name[2])\n",
    "    input_size = sbert_features.shape[1]\n",
    "    PATH = os.path.join(\"models\",name_model)\n",
    "    model = SCLIPNN(input_size,hidden_size)\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.eval()\n",
    "    output = model(sbert_features)\n",
    "    return output\n",
    "\n",
    "def get_logits(image_features, text_features):\n",
    "    # normalized features\n",
    "    if text_features.dtype == torch.int64:\n",
    "        text_features = text_features.type(torch.FloatTensor)\n",
    "    if text_features.dtype == torch.float32:\n",
    "        text_features = text_features.to(torch.float16)\n",
    "    if text_features.dtype == torch.float16:\n",
    "        text_features = text_features.to(torch.float32)\n",
    "    \n",
    "    image_features = (image_features / image_features.norm(dim=-1, keepdim=True)).to(device)\n",
    "    text_features = (text_features / text_features.norm(dim=-1, keepdim=True)).to(device)\n",
    "\n",
    "    # cosine similarity as logits\n",
    "    logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "    logit_scale = logit_scale.exp().to(device)\n",
    "    logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "    logits_per_text = logits_per_image.t()\n",
    "\n",
    "    # shape = [global_batch_size, global_batch_size]\n",
    "    return logits_per_image, logits_per_text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Images functions\n",
    "#################################\n",
    "def get_image(directory, image_id):\n",
    "    image = Image.open(os.path.join(directory, image_id))\n",
    "    return image\n",
    "\n",
    "def reshape(im):\n",
    "    print(\"This is size of original image:\",im.size, \"\\n\")\n",
    "    width, height = im.size\n",
    "    # print(\"W: {} and H: {}\".format(width, height))\n",
    "    if width > 1000 or height > 1000:\n",
    "        scale = 3\n",
    "    elif width > 500 or height > 500:\n",
    "        scale = 2\n",
    "    else:\n",
    "        scale = 1    \n",
    "    new_width = int(width / scale)\n",
    "    new_height = int(height / scale)\n",
    "    #image = preprocess(im)\n",
    "    image = fn.resize(im, size=[new_width])\n",
    "    print(\"This is size of resized image:\",image.size, \"\\n\")\n",
    "    return image\n",
    "\n",
    "def get_image_features(images, image_directory, clip_model, preprocess):\n",
    "    N = len(images)\n",
    "    count = 0\n",
    "    image_features = torch.empty(size=(N, 512))\n",
    "    for i,image_id in enumerate(images):\n",
    "        count += 1\n",
    "        im = get_image(image_directory, image_id)\n",
    "        image = preprocess(im).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            clip_image = clip_model.encode_image(image)\n",
    "            image_features[i] = clip_image\n",
    "    return image_features\n",
    "\n",
    "def get_images_and_captions(languages):\n",
    "    images_of_language = {}\n",
    "    captions_of_language = {}\n",
    "    for lang, code in languages.items():        \n",
    "        f_json = open(os.path.join(directory, \"{}_pairs.json\".format(code)), mode='r', encoding='utf-8')\n",
    "        pairs_data = json.load(f_json)\n",
    "        images = []\n",
    "        captions = []\n",
    "        for pair in pairs_data:\n",
    "            images.append(pair[\"image_id\"])\n",
    "            captions.append(pair[\"caption\"])\n",
    "        images_of_language[lang] = images\n",
    "        captions_of_language[lang] = captions\n",
    "    return images_of_language, captions_of_language\n",
    "\n",
    "def get_image_and_captions_clip_features(languages, image_directory,clip_model, preprocess):\n",
    "    images, captions = get_images_and_captions(languages)\n",
    "    images_features = {}\n",
    "    clip_features = {}\n",
    "    for lang in languages.keys():\n",
    "        images_features[lang] = get_image_features(images[lang],image_directory,clip_model, preprocess)\n",
    "        clip_features[lang] = get_clip_features(captions[lang],clip_model).to(device)\n",
    "    return images_features, clip_features, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Additional Functions\n",
    "#######################################\n",
    "def display_results(sbert_lang_performance, clip_lang_performance, sbert_lang_errors, clip_lang_errors, sbert_lang_mrr, clip_lang_mrr):\n",
    "    results = pd.DataFrame({\"SBERT\":sbert_lang_performance, \"CLIP\": clip_lang_performance,\n",
    "                        \"error SBERT\":sbert_lang_errors, \"error CLIP\":clip_lang_errors,\n",
    "                       \"MRR sbert\":sbert_lang_mrr, \"MRR clip\": clip_lang_mrr}, \n",
    "                       index=languages)\n",
    "    print(results)\n",
    "\n",
    "\n",
    "def reciprocal_rank(probs, value):\n",
    "    return float(1 / (1 + np.where(-np.sort(-probs) == value)[0][0]))\n",
    "\n",
    "\n",
    "\n",
    "def get_MRR(languages, model, sbert_model, clip_model, captions, images_features):\n",
    "    sbert_lang_performance = []\n",
    "    clip_lang_performance = []\n",
    "    sbert_lang_errors = []\n",
    "    clip_lang_errors = []\n",
    "    sbert_lang_mrr = []\n",
    "    clip_lang_mrr = []\n",
    "    vetoed = []\n",
    "    for lang, code in languages.items():\n",
    "        print(\"Lang {}\".format(lang))\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                torch_features = get_sbert_embeddings(captions[lang],sbert_model) \n",
    "                sbert_features = sbert_to_clip(torch_features,name_of_model).type(torch.float16)\n",
    "                print(\"SBERT features ready. Timestamp: {}\".format(datetime.now()))\n",
    "                clip_features = get_clip_embeddings(captions[lang],clip_model).to(device)\n",
    "                print(\"CLIP features ready. Timestamp: {}\".format(datetime.now())) \n",
    "            except:\n",
    "                print(\"Not able to tokenize in {}. Skipping language {}\".format(lang, code))\n",
    "                vetoed.append(lang)\n",
    "                continue\n",
    "\n",
    "            sbert_performance = []\n",
    "            clip_performance = []\n",
    "            sbert_errors = 0\n",
    "            clip_errors = 0\n",
    "            sbert_rr = 0\n",
    "            clip_rr = 0\n",
    "            counter = 0\n",
    "            \n",
    "            for image_feature in images_features:\n",
    "                # Get the probabilities for SBERT and CLIP\n",
    "                logits_image_sbert, logits_text_sbert = get_logits(image_feature, sbert_features)\n",
    "                logits_image_clip, logits_text_clip = get_logits(image_feature, clip_features)\n",
    "                probs_clip = logits_image_clip.softmax(dim=-1).to('cpu').numpy()\n",
    "                probs_sbert = logits_image_sbert.softmax(dim=-1).to('cpu').numpy()\n",
    "\n",
    "\n",
    "                # Append the probs to array            \n",
    "                ps = probs_sbert[counter]\n",
    "                sbert_rr += reciprocal_rank(probs_sbert,ps)\n",
    "                sbert_performance.append(ps)\n",
    "                if ps < max(probs_sbert):\n",
    "                    sbert_errors += 1\n",
    "                pc = probs_clip[counter]\n",
    "                clip_rr += reciprocal_rank(probs_clip, pc)\n",
    "                clip_performance.append(pc)\n",
    "                if pc < max(probs_clip):\n",
    "                    clip_errors += 1\n",
    "                counter += 1\n",
    "\n",
    "        # print(\"Images processed: {}\".format(counter))\n",
    "        # print(\"Classifications errors: SBERT --> {} ; CLIP --> {}\".format(sbert_errors,clip_errors))\n",
    "        sbert_lang_performance.append(round(sum(sbert_performance)/counter,4))\n",
    "        clip_lang_performance.append(round(sum(clip_performance)/counter,4))\n",
    "        sbert_lang_mrr.append(round(sbert_rr/counter,3))\n",
    "        clip_lang_mrr.append(round(clip_rr/counter,3))\n",
    "        sbert_lang_errors.append(sbert_errors)\n",
    "        clip_lang_errors.append(clip_errors)\n",
    "    \n",
    "    #print(\"Done\")\n",
    "    #print(\"Forbidden Languages: {}\".format(vetoed))\n",
    "    print(\"SBERT_LANG_PERFORMANCE: {}\".format(len(sbert_lang_performance)))\n",
    "    print(\"SBERT_LANG_PERFORMANCE: {}\".format(sbert_lang_performance))\n",
    "    return sbert_lang_performance, clip_lang_performance, sbert_lang_mrr, clip_lang_mrr, sbert_lang_errors, clip_lang_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT model loaded\n",
      "CLIP model loaded\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    with open(os.path.join(\"preprocessing\", \"config.yml\"), \"r\") as ymlfile:\n",
    "        cfg = yaml.safe_load(ymlfile)\n",
    "    directory = cfg[\"coco\"][\"out_dir\"]\n",
    "    image_directory = cfg[\"coco\"][\"image_dir\"]\n",
    "    languages = cfg[\"languages\"]\n",
    "    model_dir = cfg[\"models\"][\"model_dir\"]\n",
    "    name_of_model = 'coco_NN_900_e300_s400000.pt'\n",
    "    trained_model = os.path.join(model_dir,name_of_model)\n",
    "    sbert_model, clip_model, preprocess = get_sbert_and_clip_models()\n",
    "    images, captions = get_images_and_captions(languages)\n",
    "    images_features = get_image_features(images[\"english\"], image_directory, clip_model, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lang english\n",
      "SBERT features ready. Timestamp: 2022-12-18 14:55:47.396879\n",
      "CLIP features ready. Timestamp: 2022-12-18 14:55:47.433478\n",
      "Lang spanish\n",
      "SBERT features ready. Timestamp: 2022-12-18 14:55:47.501731\n",
      "CLIP features ready. Timestamp: 2022-12-18 14:55:47.538651\n",
      "Lang italian\n",
      "SBERT features ready. Timestamp: 2022-12-18 14:55:47.606862\n",
      "CLIP features ready. Timestamp: 2022-12-18 14:55:47.643045\n",
      "Lang german\n",
      "SBERT features ready. Timestamp: 2022-12-18 14:55:47.710358\n",
      "CLIP features ready. Timestamp: 2022-12-18 14:55:47.747500\n",
      "Lang french\n",
      "SBERT features ready. Timestamp: 2022-12-18 14:55:47.821282\n",
      "CLIP features ready. Timestamp: 2022-12-18 14:55:47.858476\n",
      "SBERT_LANG_PERFORMANCE: 5\n",
      "SBERT_LANG_PERFORMANCE: [0.3581, 0.3269, 0.3378, 0.3422, 0.3395]\n",
      "          SBERT    CLIP  error SBERT  error CLIP  MRR sbert  MRR clip\n",
      "english  0.3581  0.4355            1           1       0.95     0.950\n",
      "spanish  0.3269  0.2155            1           2       0.95     0.883\n",
      "italian  0.3378  0.2357            2           1       0.90     0.950\n",
      "german   0.3422  0.2294            1           2       0.95     0.870\n",
      "french   0.3395  0.2701            2           1       0.90     0.950\n"
     ]
    }
   ],
   "source": [
    "model = load_model(trained_model,sbert_model)\n",
    "sbert_per, clip_per, sbert_MRR, clip_MRR, sbert_errors, clip_errors = get_MRR(languages,model,sbert_model,clip_model,captions, images_features)\n",
    "display_results(sbert_per,clip_per,sbert_errors, clip_errors,sbert_MRR,clip_MRR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
