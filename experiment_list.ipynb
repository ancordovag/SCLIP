{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72f9a506-ce94-4257-a35e-2ebe95b653c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from networks import SCLIPNN\n",
    "import clip\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torchvision.transforms.functional as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37af0fba-1bc3-4784-9b5c-72713397d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "with open(\"config.yml\", \"r\") as ymlfile:\n",
    "    cfg = yaml.safe_load(ymlfile)\n",
    "directory = cfg[\"dataset\"][\"dirname\"]\n",
    "image_directory = cfg[\"dataset\"][\"images\"]\n",
    "languages = cfg[\"languages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c1a6681-6ff0-4d37-b1a8-3fb5e6ce3b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2') #distiluse-base-multilingual-cased-v1') #\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0be2a0cb-b450-44b9-9073-235f44cd1001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(im):\n",
    "    print(\"This is size of original image:\",im.size, \"\\n\")\n",
    "    width, height = im.size\n",
    "    # print(\"W: {} and H: {}\".format(width, height))\n",
    "    if width > 1000 or height > 1000:\n",
    "        scale = 3\n",
    "    elif width > 500 or height > 500:\n",
    "        scale = 2\n",
    "    else:\n",
    "        scale = 1    \n",
    "    new_width = int(width / scale)\n",
    "    new_height = int(height / scale)\n",
    "    #image = preprocess(im)\n",
    "    image = fn.resize(im, size=[new_width])\n",
    "    print(\"This is size of resized image:\",image.size, \"\\n\")\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d2fab58c-85bd-4007-a2ee-55b52de3fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logits(image_features, text_features):\n",
    "    # normalized features\n",
    "    if text_features.dtype == torch.int64:\n",
    "        text_features = text_features.type(torch.FloatTensor)\n",
    "    image_features = (image_features / image_features.norm(dim=-1, keepdim=True)).to(device)\n",
    "    text_features = (text_features / text_features.norm(dim=-1, keepdim=True)).to(device)\n",
    "\n",
    "    # cosine similarity as logits\n",
    "    logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "    logit_scale = logit_scale.exp().to(device)\n",
    "    logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "    logits_per_text = logits_per_image.t()\n",
    "\n",
    "    # shape = [global_batch_size, global_batch_size]\n",
    "    return logits_per_image, logits_per_text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea774706-c674-43b6-a025-64603064008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sbert_to_clip(sbert_features, name_model):\n",
    "    splitted_name = name_model.split(\"_\")\n",
    "    hidden_size = int(splitted_name[2])\n",
    "    input_size = sbert_features.shape[1]\n",
    "    PATH = os.path.join(\"models\",name_model)\n",
    "    model = SCLIPNN(input_size,hidden_size)\n",
    "    print(\"DEBUG 4\")\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    print(\"DEBUG 5\")\n",
    "    model.eval()\n",
    "    output = model(sbert_features)\n",
    "    print(\"DEBUG 6\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab389a91-26a4-458f-9715-2b1f530af6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(directory, image_id):\n",
    "    image = Image.open(directory + image_id)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2fb20e4f-c0a3-47d7-b859-19e72482e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank(probs, value):\n",
    "    N = len(probs)\n",
    "    copy_probs = list(probs.copy())\n",
    "    for i in range(N):\n",
    "        max_value = max(copy_probs)\n",
    "        if max_value == value:\n",
    "            return 1/(i + 1)\n",
    "        else:\n",
    "            copy_probs.remove(max_value)\n",
    "    return 1/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb1f3a56-0a4b-4fbd-9c5b-05160c0069d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multilingual_experiment(name_of_model, directory, languages):\n",
    "    sbert_lang_performance = []\n",
    "    clip_lang_performance = []\n",
    "    sbert_lang_errors = []\n",
    "    clip_lang_errors = []\n",
    "    sbert_lang_mrr = []\n",
    "    clip_lang_mrr = []\n",
    "    vetoed = []\n",
    "    for lang, code in languages.items():\n",
    "        #print(\"Processing captions in \"+ lang +\"...\")\n",
    "        f_json =  open(directory + code + \"_pairs.json\",mode='r',encoding='utf-8')\n",
    "        pairs_data = json.load(f_json)\n",
    "        images = []\n",
    "        captions = []\n",
    "        for pair in pairs_data:\n",
    "            images.append(pair[\"image_id\"])\n",
    "            captions.append(pair[\"caption\"])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                torch_features = torch.from_numpy(sbert_model.encode(captions))\n",
    "                print(\"DBUG 1\")\n",
    "                sbert_features = sbert_to_clip(torch_features,name_of_model).type(torch.float16)\n",
    "                print(\"DBUG 2\")\n",
    "                tokenized_features = clip.tokenize(captions).to(device)\n",
    "                print(\"DBUG 3\")\n",
    "                clip_features = clip_model.encode_text(tokenized_features)\n",
    "            except:\n",
    "                print(\"Not able to tokenize in {}. Skipping language {}\".format(lang, code))\n",
    "                vetoed.append(lang)\n",
    "                continue\n",
    "\n",
    "            sbert_performance = []\n",
    "            clip_performance = []\n",
    "            sbert_errors = 0\n",
    "            clip_errors = 0\n",
    "            sbert_rr = 0\n",
    "            clip_rr = 0\n",
    "            counter = 0\n",
    "\n",
    "            for image_id in images:\n",
    "                # Get the encoding of the image\n",
    "                im = get_image(image_directory, image_id)\n",
    "                image = preprocess(im).unsqueeze(0).to(device)\n",
    "                image_features = clip_model.encode_image(image)\n",
    "\n",
    "                # Get the probabilities for SBERT and CLIP\n",
    "                logits_image_sbert, logits_text_sbert = get_logits(image_features, sbert_features)\n",
    "                logits_image_clip, logits_text_clip = get_logits(image_features, clip_features)\n",
    "                probs_clip = logits_image_clip.softmax(dim=-1).cpu().numpy()\n",
    "                probs_sbert = logits_image_sbert.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "                # Append the probs to array            \n",
    "                ps = probs_sbert[0][counter]\n",
    "                sbert_rr += reciprocal_rank(probs_sbert[0],ps)\n",
    "                sbert_performance.append(ps)\n",
    "                if ps < max(probs_sbert[0]):\n",
    "                    sbert_errors += 1\n",
    "                pc = probs_clip[0][counter]\n",
    "                clip_rr += reciprocal_rank(probs_clip[0],pc)\n",
    "                clip_performance.append(pc)\n",
    "                if pc < max(probs_clip[0]):\n",
    "                    clip_errors += 1\n",
    "                counter += 1\n",
    "\n",
    "        # print(\"Images processed: {}\".format(counter))\n",
    "        # print(\"Classifications errors: SBERT --> {} ; CLIP --> {}\".format(sbert_errors,clip_errors))\n",
    "        sbert_lang_performance.append(round(sum(sbert_performance)/counter,6))\n",
    "        clip_lang_performance.append(round(sum(clip_performance)/counter,6))\n",
    "        sbert_lang_mrr.append(round(sbert_rr/counter,4))\n",
    "        clip_lang_mrr.append(round(clip_rr/counter,4))\n",
    "        sbert_lang_errors.append(sbert_errors)\n",
    "        clip_lang_errors.append(clip_errors)\n",
    "    #print(\"Done\")\n",
    "    #print(\"Forbidden Languages: {}\".format(vetoed))\n",
    "    return sbert_lang_mrr, clip_lang_mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04d73b37-73ee-4ba1-a3fe-5725c43d62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(languages, sbert_lang_mrr, clip_lang_mrr):\n",
    "    X_axis = np.arange(len(languages.keys()))\n",
    "    figure_name = plt.figure(figsize=(20, 8))\n",
    "    plt.bar(X_axis-0.2, sbert_lang_mrr, 0.4, color = 'blue', edgecolor = 'black', capsize=7, label='SBERT MRR')\n",
    "    plt.bar(X_axis+0.2, clip_lang_mrr, 0.4, color = 'red', edgecolor = 'black', capsize=7, label='CLIP MRR')\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.xticks(X_axis, languages.keys())\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "830874c7-3dc4-4c8e-94c3-608063e22e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: {'English': 'en', 'Spanish': 'es', 'Italian': 'it', 'German': 'de', 'French': 'fr'}\n",
      "DBUG 1\n",
      "DEBUG 4\n",
      "DEBUG 5\n",
      "DEBUG 6\n",
      "DBUG 2\n",
      "DBUG 3\n",
      "DBUG 1\n",
      "DEBUG 4\n",
      "DEBUG 5\n",
      "DEBUG 6\n",
      "DBUG 2\n",
      "DBUG 3\n",
      "DBUG 1\n",
      "DEBUG 4\n",
      "DEBUG 5\n",
      "DEBUG 6\n",
      "DBUG 2\n",
      "DBUG 3\n",
      "DBUG 1\n",
      "DEBUG 4\n",
      "DEBUG 5\n",
      "DEBUG 6\n",
      "DBUG 2\n",
      "DBUG 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-19b26e725cef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Languages: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msbert_lang_mrr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_lang_mrr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_multilingual_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Name of the model: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SBERT performance: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msbert_lang_mrr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-433b341812dd>\u001b[0m in \u001b[0;36mrun_multilingual_experiment\u001b[0;34m(name_of_model, directory, languages)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# Get the encoding of the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SCLIP/venv/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SCLIP/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SCLIP/venv/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \"\"\"\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SCLIP/venv/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    417\u001b[0m             )\n\u001b[1;32m    418\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SCLIP/venv/lib/python3.6/site-packages/torchvision/transforms/functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mnew_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_short\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_long\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnew_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_short\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmax_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SCLIP/venv/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   1987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1989\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreducing_gap\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresample\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mNEAREST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SCLIP/venv/lib/python3.6/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = [x for x in os.listdir('models') if 'e250' in x]\n",
    "print(\"Languages: {}\".format(languages))\n",
    "for model in models:\n",
    "    sbert_lang_mrr, clip_lang_mrr = run_multilingual_experiment(model, directory, languages)\n",
    "    print(\"Name of the model: {}\".format(model))    \n",
    "    print(\"SBERT performance: {}\".format(sbert_lang_mrr))\n",
    "    print(\"CLIP performance: {}\".format(clip_lang_mrr))\n",
    "    show_plot(languages, sbert_lang_mrr, clip_lang_mrr)\n",
    "    print('-'*100)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "869665bb-9fda-4137-ad1c-0ed9a3e42104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SBERT</th>\n",
       "      <th>CLIP</th>\n",
       "      <th>error SBERT</th>\n",
       "      <th>error CLIP</th>\n",
       "      <th>MRR sbert</th>\n",
       "      <th>MRR clip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>English</th>\n",
       "      <td>0.063924</td>\n",
       "      <td>0.091794</td>\n",
       "      <td>25</td>\n",
       "      <td>18</td>\n",
       "      <td>0.7825</td>\n",
       "      <td>0.8511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Spanish</th>\n",
       "      <td>0.057844</td>\n",
       "      <td>0.039343</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>0.7374</td>\n",
       "      <td>0.6481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italian</th>\n",
       "      <td>0.058054</td>\n",
       "      <td>0.034438</td>\n",
       "      <td>33</td>\n",
       "      <td>45</td>\n",
       "      <td>0.7144</td>\n",
       "      <td>0.5564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>German</th>\n",
       "      <td>0.059676</td>\n",
       "      <td>0.033382</td>\n",
       "      <td>27</td>\n",
       "      <td>44</td>\n",
       "      <td>0.7567</td>\n",
       "      <td>0.5554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>French</th>\n",
       "      <td>0.062021</td>\n",
       "      <td>0.042905</td>\n",
       "      <td>29</td>\n",
       "      <td>33</td>\n",
       "      <td>0.7523</td>\n",
       "      <td>0.6816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            SBERT      CLIP  error SBERT  error CLIP  MRR sbert  MRR clip\n",
       "English  0.063924  0.091794           25          18     0.7825    0.8511\n",
       "Spanish  0.057844  0.039343           30          38     0.7374    0.6481\n",
       "Italian  0.058054  0.034438           33          45     0.7144    0.5564\n",
       "German   0.059676  0.033382           27          44     0.7567    0.5554\n",
       "French   0.062021  0.042905           29          33     0.7523    0.6816"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({\"SBERT\":sbert_lang_performance, \"CLIP\": clip_lang_performance,\n",
    "                        \"error SBERT\":sbert_lang_errors, \"error CLIP\":clip_lang_errors,\n",
    "                       \"MRR sbert\":sbert_lang_mrr, \"MRR clip\": clip_lang_mrr}, \n",
    "                       index=languages)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80df23b-e068-4782-8277-e60d288fb690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e181d24a-0790-4cbd-995c-9f4472336363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: {'English': 'en', 'Spanish': 'es', 'Italian': 'it', 'German': 'de', 'French': 'fr'}\n",
      "DBUG 1\n",
      "DEBUG 4\n",
      "Not able to tokenize in English. Skipping language en\n",
      "DBUG 1\n",
      "DEBUG 4\n",
      "Not able to tokenize in Spanish. Skipping language es\n",
      "DBUG 1\n",
      "DEBUG 4\n",
      "Not able to tokenize in Italian. Skipping language it\n",
      "DBUG 1\n",
      "DEBUG 4\n",
      "Not able to tokenize in German. Skipping language de\n",
      "DBUG 1\n",
      "DEBUG 4\n",
      "Not able to tokenize in French. Skipping language fr\n",
      "Name of the model: coco_NN_900_e300_s200000.pt\n",
      "SBERT performance: []\n",
      "CLIP performance: []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: objects cannot be broadcast to a single shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-a497b3b6ece5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SBERT performance: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msbert_lang_mrr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CLIP performance: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_lang_mrr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mshow_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msbert_lang_mrr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_lang_mrr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-d9cae7601eef>\u001b[0m in \u001b[0;36mshow_plot\u001b[0;34m(languages, sbert_lang_mrr, clip_lang_mrr)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mX_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfigure_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_axis\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msbert_lang_mrr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'black'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SBERT MRR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_axis\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_lang_mrr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'red'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'black'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CLIP MRR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SCLIP/venv/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(x, height, width, bottom, align, data, **kwargs)\u001b[0m\n\u001b[1;32m   2487\u001b[0m     return gca().bar(\n\u001b[1;32m   2488\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbottom\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbottom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m         **({\"data\": data} if data is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SCLIP/venv/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1447\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SCLIP/venv/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mbar\u001b[0;34m(self, x, height, width, bottom, align, **kwargs)\u001b[0m\n\u001b[1;32m   2430\u001b[0m         x, height, width, y, linewidth = np.broadcast_arrays(\n\u001b[1;32m   2431\u001b[0m             \u001b[0;31m# Make args iterable too.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2432\u001b[0;31m             np.atleast_1d(x), height, width, y, linewidth)\n\u001b[0m\u001b[1;32m   2433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2434\u001b[0m         \u001b[0;31m# Now that units have been converted, set the tick locations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbroadcast_arrays\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/SCLIP/venv/lib/python3.6/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36mbroadcast_arrays\u001b[0;34m(subok, *args)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubok\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_broadcast_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SCLIP/venv/lib/python3.6/site-packages/numpy/lib/stride_tricks.py\u001b[0m in \u001b[0;36m_broadcast_shape\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# use the old-iterator because np.nditer does not handle size 0 arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;31m# consistently\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;31m# unfortunately, it cannot handle 32 or more arguments directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = [x for x in os.listdir('models') if 'e300' in x]\n",
    "print(\"Languages: {}\".format(languages))\n",
    "for model in models:\n",
    "    sbert_lang_mrr, clip_lang_mrr = run_multilingual_experiment(model, directory, languages)\n",
    "    print(\"Name of the model: {}\".format(model))    \n",
    "    print(\"SBERT performance: {}\".format(sbert_lang_mrr))\n",
    "    print(\"CLIP performance: {}\".format(clip_lang_mrr))\n",
    "    show_plot(languages, sbert_lang_mrr, clip_lang_mrr)\n",
    "    print('-'*100)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee703275-e89a-4858-bb3f-304ae4708502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
